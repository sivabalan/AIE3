{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Embedding Models\n",
        "\n",
        "In the following Notebook we will be exploring one of the most powerful techniques to take your single-domain RAG pipelines to the next level...\n",
        "\n",
        "**Fine-tuning the Embeddings Model!**\n",
        "\n",
        "---\n",
        "In this notebook, we'll worth through the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Dependencies and Boilerplate\n",
        "  2. Loading Data\n",
        "  3. Constructing a Fine-tuning Dataset\n",
        "  4. Fine-tuning `snowflake-arctic-embed-m`\n",
        "  5. Evaluating our Retriever"
      ],
      "metadata": {
        "id": "ckbbj5diaHkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Dependencies and Boilerplate\n",
        "\n",
        "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
        "\n",
        "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
      ],
      "metadata": {
        "id": "-NkSaurzbpyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nest Asyncio"
      ],
      "metadata": {
        "id": "9c_EUibmcDU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zq-6s7LbPnKH"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "R8uFz8RVcFFu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ulZIBA1ZoSsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ec4eac-d9d6-4fa8-a1ad-20ff823632e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m372.0/372.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m987.7/987.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m328.6/328.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.0/135.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pymupdf faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GFD7B-tOCrx",
        "outputId": "8c36d3ff-349b-401f-b522-9c2d0420efc4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provide OpenAI API Key"
      ],
      "metadata": {
        "id": "0FM-eUlrcI8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA_mlurVqtrp",
        "outputId": "f5cd4d89-346f-4a16-878f-5a4834fb331e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZ217gCDVTr"
      },
      "source": [
        "## Task 2: Loading Data\n",
        "\n",
        "The data can be found in [this GitHub repo](https://github.com/AI-Maker-Space/DataRepository/tree/main/high-performance-rag)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AI-Maker-Space/DataRepository.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9z9tZVAedkk",
        "outputId": "ab9fb8fe-d2d6-4d48-b08b-7684ef601bbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DataRepository'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 78 (delta 21), reused 28 (delta 8), pack-reused 8\u001b[K\n",
            "Receiving objects: 100% (78/78), 69.86 MiB | 28.35 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DataRepository"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr3uSmKQPAWG",
        "outputId": "f74df5ab-6cf6-4b66-e630-ef343bc47c12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DataRepository\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "training_documents_loaded = PyMuPDFLoader(\"MuskComplaint.pdf\")"
      ],
      "metadata": {
        "id": "DHJhTzsvN75t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 250,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len\n",
        ")"
      ],
      "metadata": {
        "id": "NsPrOOqXOsNX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_documents = text_splitter.split_documents(training_documents_loaded.load())"
      ],
      "metadata": {
        "id": "OMYPX6N6Os8M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAozuMoNOvnp",
        "outputId": "a7907273-15e2-4a0b-8778-07351b2f1667"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "481"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "id_set = set()\n",
        "\n",
        "for document in training_documents:\n",
        "  id = str(uuid.uuid4())\n",
        "  while id in id_set:\n",
        "    id = uuid.uuid4()\n",
        "  id_set.add(id)\n",
        "  document.metadata[\"id\"] = id"
      ],
      "metadata": {
        "id": "AwyIForybIpo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_split_documents = training_documents[:300]"
      ],
      "metadata": {
        "id": "MTS4GTSEcnG4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_split_documents = training_documents[300:350]"
      ],
      "metadata": {
        "id": "Rxv2OlO6Oxjz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_split_documents = training_documents[350:400]"
      ],
      "metadata": {
        "id": "I0RmbqEWS-SR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlvKbONDWvQ"
      },
      "source": [
        "## Task 3: Constructing a Fine-tuning Dataset\n",
        "\n",
        "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4o-mini` (released [today](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)).\n",
        "\n",
        "The basic idea here is straightforward enough:\n",
        "\n",
        "1. We look at a document\n",
        "2. We generate questions that could be answered by that node\n",
        "\n",
        "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "qa_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "_EWfmIscMrvg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
      ],
      "metadata": {
        "id": "8-hLnsSB6Y-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_prompt = \"\"\"\\\n",
        "Given the following context, you must generate questions based on only the provided context.\n",
        "\n",
        "You are to generate {n_questions} questions which should be provided in the following format:\n",
        "\n",
        "1. QUESTION #1\n",
        "2. QUESTION #2\n",
        "...\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
      ],
      "metadata": {
        "id": "diEWcw00NMSj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a simple chain to query the LLM!"
      ],
      "metadata": {
        "id": "u87Izpgm6_fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_generation_chain = qa_prompt_template | qa_chat_model"
      ],
      "metadata": {
        "id": "ggl9SSjiNbpG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a lot going on in this function - let's take a deeper look:\n",
        "\n",
        "1. First, we provide a list of documents and a number of questions\n",
        "2. We, for each document in our list, generate `n_questions` of questions.\n",
        "3. We then associate those questions and contexts via a `UUID`.\n",
        "\n",
        "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
      ],
      "metadata": {
        "id": "4duvHirh7DQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def create_questions(documents, n_questions):\n",
        "  questions = {}\n",
        "  relevant_docs = {}\n",
        "  for document in tqdm.tqdm(documents):\n",
        "    document_content = {\"context\" : document.page_content, \"questions\" : []}\n",
        "    questions_generated = question_generation_chain.invoke({\"context\": document.page_content, \"n_questions\": n_questions})\n",
        "    for question in questions_generated.content.split(\"\\n\"):\n",
        "      question_id = str(uuid.uuid4())\n",
        "      questions[question_id] = \"\".join(question.split(\".\")[1:]).strip()\n",
        "      relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
        "  return questions, relevant_docs"
      ],
      "metadata": {
        "id": "KbzkPTPVPH-I"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the function to generate training, validation, and test data."
      ],
      "metadata": {
        "id": "_FSTG0bb7w73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_questions, training_relevant_contexts = create_questions(training_split_documents, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ex3LzsfPQEM",
        "outputId": "004198da-5f7c-4c00-8ebf-edd494bfd5ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [03:28<00:00,  1.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_questions, val_relevant_contexts = create_questions(val_split_documents, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIZm4CqGVzBx",
        "outputId": "02b19626-17b7-4f9b-c2b6-2096432a8113"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:33<00:00,  1.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions, test_relevant_contexts = create_questions(test_split_documents, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6qUHg9sV2_y",
        "outputId": "54144261-0f88-4259-8531-899918babece"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:33<00:00,  1.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll save each dataset for use late - if it's required.\n",
        "\n",
        "> NOTE: These datasets will be provided in the repository in case you run into any issues with the data generation steps or you wish to save API calls."
      ],
      "metadata": {
        "id": "F0U959IN71dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
        "\n",
        "train_dataset = {\n",
        "    \"questions\" : training_questions,\n",
        "    \"relevant_contexts\" : training_relevant_contexts,\n",
        "    \"corpus\" : training_corpus\n",
        "}\n",
        "\n",
        "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(train_dataset, f)"
      ],
      "metadata": {
        "id": "iF6IFFq9VsNu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
        "\n",
        "val_dataset = {\n",
        "    \"questions\" : val_questions,\n",
        "    \"relevant_contexts\" : val_relevant_contexts,\n",
        "    \"corpus\" : val_corpus\n",
        "}\n",
        "\n",
        "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(val_dataset, f)"
      ],
      "metadata": {
        "id": "PqF9WaueV-V8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
        "\n",
        "test_dataset = {\n",
        "    \"questions\" : test_questions,\n",
        "    \"relevant_contexts\" : test_relevant_contexts,\n",
        "    \"corpus\" : train_corpus\n",
        "}\n",
        "\n",
        "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(test_dataset, f)"
      ],
      "metadata": {
        "id": "0DSQ7WMnWAu6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Fine-tuning `snowflake-arctic-embed-m`\n",
        "\n",
        "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
        "\n",
        "We'll be using Snowflake's [`snowflake-arctic-embed-m`](https://huggingface.co/Snowflake/snowflake-arctic-embed-m) as a base embeddings model.\n",
        "\n",
        "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!"
      ],
      "metadata": {
        "id": "vAwklqQCgVi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AXzVHP3v1Cno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02028873-e939-4562-ccc8-c24e141939c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU sentence_transformers datasets pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-m\"\n",
        "model = SentenceTransformer(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-PGsQB7Xo6V",
        "outputId": "41ddc960-c958-46f0-8a7a-6926983aa099"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll grab some necessary imports from `sentence_transformers` and `torch`."
      ],
      "metadata": {
        "id": "4ztG07iB8CFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import InputExample"
      ],
      "metadata": {
        "id": "B-WbpuUWYFJr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're using a toy batch size here to reflect the limited number of examples we have.\n",
        "\n",
        "Ideally we'd want to use a much larger batch size. (~64+)"
      ],
      "metadata": {
        "id": "AJtPPlck8HBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 20"
      ],
      "metadata": {
        "id": "8Lokhy6KYHAv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's move our dataset into the expected format for training."
      ],
      "metadata": {
        "id": "b-6DT8hc8PmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = train_dataset['corpus']\n",
        "queries = train_dataset['questions']\n",
        "relevant_docs = train_dataset['relevant_contexts']\n",
        "\n",
        "examples = []\n",
        "for query_id, query in queries.items():\n",
        "    doc_id = relevant_docs[query_id][0]\n",
        "    text = corpus[doc_id]\n",
        "    example = InputExample(texts=[query, text])\n",
        "    examples.append(example)"
      ],
      "metadata": {
        "id": "JJk37zQsYJ4P"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a `torch` `DataLoader`!"
      ],
      "metadata": {
        "id": "OjFx7KHI8TL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader(\n",
        "    examples, batch_size=BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "tiizmeIqZ_-w"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up, we'll prepare our loss function!\n",
        "\n",
        "The loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py)."
      ],
      "metadata": {
        "id": "_vA8rzlX8XbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "loss = losses.MultipleNegativesRankingLoss(model)"
      ],
      "metadata": {
        "id": "OLEO55iSaBiN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does this loss work? Can you explain what samples are positive vs. what samples are negative?"
      ],
      "metadata": {
        "id": "_S688xAk8sip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set-up our evaluator.\n",
        "\n",
        "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
      ],
      "metadata": {
        "id": "QKxRuXfH844c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "corpus = val_dataset['corpus']\n",
        "queries = val_dataset['questions']\n",
        "relevant_docs = val_dataset['relevant_contexts']\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
      ],
      "metadata": {
        "id": "f0hAFwUyaHQG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
      ],
      "metadata": {
        "id": "MYfap_ct8-bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "svZG0pBHiQr6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's training time!\n",
        "\n",
        "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
      ],
      "metadata": {
        "id": "wxitWoNX9DwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader, loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='finetuned_arctic',\n",
        "    show_progress_bar=True,\n",
        "    evaluator=evaluator,\n",
        "    evaluation_steps=50,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332,
          "referenced_widgets": [
            "b99ccc41a6ff4383bf1781e2fbb2591e",
            "3701922ddbc147af910500d8bb33f6d8",
            "ba5e99343a5e4c0db92a99f14b34a355",
            "7d12f352771742d5a9f4f6ba1fdb28f7",
            "eef701e52c0447c093b7461200acde71",
            "4cbc442b299f4394be984e857b1c9637",
            "05aed3d3076b47f6b668756f128b8e96",
            "66b4c731c11f4fd3badfb17f2c5fed4a",
            "a4b2f6b5744440b89886e2c34a1eca2d",
            "8ead19d34b6543fd9d054901101f0df9",
            "db63a9a43a034cf1b4c5113b011e1f2f"
          ]
        },
        "id": "aDhUHZY-iR09",
        "outputId": "e5403a2c-22ab-4495-e4b4-9b7dffdbe9dd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:34, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cosine Accuracy@1</th>\n",
              "      <th>Cosine Accuracy@3</th>\n",
              "      <th>Cosine Accuracy@5</th>\n",
              "      <th>Cosine Accuracy@10</th>\n",
              "      <th>Cosine Precision@1</th>\n",
              "      <th>Cosine Precision@3</th>\n",
              "      <th>Cosine Precision@5</th>\n",
              "      <th>Cosine Precision@10</th>\n",
              "      <th>Cosine Recall@1</th>\n",
              "      <th>Cosine Recall@3</th>\n",
              "      <th>Cosine Recall@5</th>\n",
              "      <th>Cosine Recall@10</th>\n",
              "      <th>Cosine Ndcg@10</th>\n",
              "      <th>Cosine Mrr@10</th>\n",
              "      <th>Cosine Map@100</th>\n",
              "      <th>Dot Accuracy@1</th>\n",
              "      <th>Dot Accuracy@3</th>\n",
              "      <th>Dot Accuracy@5</th>\n",
              "      <th>Dot Accuracy@10</th>\n",
              "      <th>Dot Precision@1</th>\n",
              "      <th>Dot Precision@3</th>\n",
              "      <th>Dot Precision@5</th>\n",
              "      <th>Dot Precision@10</th>\n",
              "      <th>Dot Recall@1</th>\n",
              "      <th>Dot Recall@3</th>\n",
              "      <th>Dot Recall@5</th>\n",
              "      <th>Dot Recall@10</th>\n",
              "      <th>Dot Ndcg@10</th>\n",
              "      <th>Dot Mrr@10</th>\n",
              "      <th>Dot Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.194000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.937403</td>\n",
              "      <td>0.920667</td>\n",
              "      <td>0.921576</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.194000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.937403</td>\n",
              "      <td>0.920667</td>\n",
              "      <td>0.921576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.316667</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.948969</td>\n",
              "      <td>0.935667</td>\n",
              "      <td>0.936576</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.316667</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.948969</td>\n",
              "      <td>0.935667</td>\n",
              "      <td>0.936576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.316667</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.950278</td>\n",
              "      <td>0.937333</td>\n",
              "      <td>0.938242</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.316667</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.950278</td>\n",
              "      <td>0.937333</td>\n",
              "      <td>0.938242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.950972</td>\n",
              "      <td>0.938167</td>\n",
              "      <td>0.939076</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.950972</td>\n",
              "      <td>0.938167</td>\n",
              "      <td>0.939076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.950972</td>\n",
              "      <td>0.938167</td>\n",
              "      <td>0.939076</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.950972</td>\n",
              "      <td>0.938167</td>\n",
              "      <td>0.939076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.952281</td>\n",
              "      <td>0.939833</td>\n",
              "      <td>0.940742</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.952281</td>\n",
              "      <td>0.939833</td>\n",
              "      <td>0.940742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.950972</td>\n",
              "      <td>0.938167</td>\n",
              "      <td>0.939076</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.950972</td>\n",
              "      <td>0.938167</td>\n",
              "      <td>0.939076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b99ccc41a6ff4383bf1781e2fbb2591e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Evaluating our Retriever\n",
        "\n",
        "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
        "\n",
        "We'll start with some basic imports."
      ],
      "metadata": {
        "id": "6bo0zW5k9Poq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "Vq-2oqU0wHFr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll define a function that will help us evaluate our retrieval process.\n",
        "\n",
        "> NOTE: We're assuming 1 correct document in a \"hit\"."
      ],
      "metadata": {
        "id": "5jD0qrIh9X8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_openai(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "  corpus = dataset['corpus']\n",
        "  questions = dataset['questions']\n",
        "  relevant_docs = dataset['relevant_contexts']\n",
        "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
        "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "  eval_results = []\n",
        "  for id, question in tqdm.tqdm(questions.items()):\n",
        "    retrieved_nodes = retriever.invoke(question)\n",
        "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
        "    expected_id = relevant_docs[id][0]\n",
        "    is_hit = expected_id in retrieved_ids\n",
        "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
        "\n",
        "  return eval_results"
      ],
      "metadata": {
        "id": "0713_3cowX4q"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üèóÔ∏è Activity #1:\n",
        "\n",
        "Describe what the function is doing in detail!"
      ],
      "metadata": {
        "id": "gsuTpRA69fEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that's left to do is evaluate, we'll evaluate our model against:\n",
        "\n",
        "1. OpenAI's closed source `text-embedding-3-small`\n",
        "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-m`.\n",
        "\n",
        "Let's see how it stacks up!"
      ],
      "metadata": {
        "id": "hOr49m4O9lxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `text-embedding-3-small`"
      ],
      "metadata": {
        "id": "ijaeYpf593IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "te3_results = evaluate_openai(test_dataset, te3_openai)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyY3PztaxnU3",
        "outputId": "4c18f521-fc11-4ed5-c673-e22c3b02dece"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:19<00:00,  5.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "te3_results_df = pd.DataFrame(te3_results)"
      ],
      "metadata": {
        "id": "kkyW90TCxx_i"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MscVRdNCylJ-",
        "outputId": "6759f80a-da27-4623-eb3a-ea1b4ca64394"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.98"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Snowflake/snowflake-arctic-embed-m` (base)"
      ],
      "metadata": {
        "id": "4Ra-mh0L96dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-m\")\n",
        "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEskxwvFypHe",
        "outputId": "7eca3d0b-cc48-40b2-c78e-e6ac5437a93d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 81.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
      ],
      "metadata": {
        "id": "KlKgiXTWzMTg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV5vJWrJzOhc",
        "outputId": "ad9559a3-bbd4-4e25-96d6-bb5caaab9689"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Snowflake/snowflake-arctic-embed-m` (fine-tuned)"
      ],
      "metadata": {
        "id": "lcR3-0s19_lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic\")\n",
        "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilse1LduzP1i",
        "outputId": "c1091e41-7962-43d8-a6ad-b0b3ba6b251e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 80.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_results_df = pd.DataFrame(finetune_results)"
      ],
      "metadata": {
        "id": "xxhZPqkNzZlh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
        "finetune_hit_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4thAK2BXzaj6",
        "outputId": "95b1eb46-4b63-4068-bc67-67f1e741b610"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "As you can see - with only a few hundred data points, we're able to increase our embedding model to outperform even OpenAI's closed source solution!"
      ],
      "metadata": {
        "id": "Y-aKOB1m-Bi8"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b99ccc41a6ff4383bf1781e2fbb2591e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3701922ddbc147af910500d8bb33f6d8",
              "IPY_MODEL_ba5e99343a5e4c0db92a99f14b34a355",
              "IPY_MODEL_7d12f352771742d5a9f4f6ba1fdb28f7"
            ],
            "layout": "IPY_MODEL_eef701e52c0447c093b7461200acde71"
          }
        },
        "3701922ddbc147af910500d8bb33f6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cbc442b299f4394be984e857b1c9637",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_05aed3d3076b47f6b668756f128b8e96",
            "value": "Computing‚Äáwidget‚Äáexamples:‚Äá‚Äá‚Äá0%"
          }
        },
        "ba5e99343a5e4c0db92a99f14b34a355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66b4c731c11f4fd3badfb17f2c5fed4a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4b2f6b5744440b89886e2c34a1eca2d",
            "value": 1
          }
        },
        "7d12f352771742d5a9f4f6ba1fdb28f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ead19d34b6543fd9d054901101f0df9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_db63a9a43a034cf1b4c5113b011e1f2f",
            "value": "‚Äá0/1‚Äá[00:00&lt;?,‚Äá?example/s]"
          }
        },
        "eef701e52c0447c093b7461200acde71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4cbc442b299f4394be984e857b1c9637": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05aed3d3076b47f6b668756f128b8e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66b4c731c11f4fd3badfb17f2c5fed4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4b2f6b5744440b89886e2c34a1eca2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ead19d34b6543fd9d054901101f0df9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db63a9a43a034cf1b4c5113b011e1f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}